{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamento de dados GLM para a criação de uma composição audiovisual  "
      ],
      "metadata": {
        "id": "VmRQCFlDoeB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para que serve?"
      ],
      "metadata": {
        "id": "qMnTKPhBpo1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O códifo abaixo manipula para dados do satélite do tipo GLM (geostationary lightning mapper), que é um mapeador de relâmpagos em órbita geoestacionária. Ele possuiu o formato NetCDF e é feita uma transformação dos dados, colocando-os em um csv, com as informações de latitude, longitude e data/horário do acontecimento, essa planilha passa por um tratamento para podermos usá-la em uma composição audiovisual do tipo p5js, que utiliza dados em tempo real em até 24 horas da execução da requisição."
      ],
      "metadata": {
        "id": "47iQwSSHp2uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliotecas necessárias:"
      ],
      "metadata": {
        "id": "7oOQvbj-qiX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from netCDF4 import Dataset                           # Read / Write NetCDF4 files\n",
        "from datetime import timedelta, datetime, date        # Basic Dates and time types\n",
        "import os                                             # Miscellaneous operating system interfaces\n",
        "from osgeo import gdal                                # Python bindings for GDAL\n",
        "import boto3                                          # Amazon Web Services (AWS) SDK for Python\n",
        "from botocore import UNSIGNED                         # boto3 config\n",
        "from botocore.config import Config                    # boto3 config\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pytz                                           # fuso horário\n",
        "import csv\n",
        "import random\n",
        "gdal.PushErrorHandler('CPLQuietErrorHandler')         # Ignore GDAL warnings"
      ],
      "metadata": {
        "id": "T3HzjGcEqooj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foram acrescentadas as bibliotecas pickle, random e csv para podermos realizar o tratamento."
      ],
      "metadata": {
        "id": "mKLhdbUXqvJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicando a lógica"
      ],
      "metadata": {
        "id": "dG2kC6SHrLh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiramente é deletado todos os arquivos já existentes nas pastas Samples (com as imagens e arquivos por mapeamento de área) e GLM (que contém as tabelas com todos os arquivos), já que, por execução são criados em média 25 arquivos na primeira pasta e longas tabelas na pasta GLM."
      ],
      "metadata": {
        "id": "9FKbmV_5rqu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deletar_arquivos_pasta(pasta):\n",
        "    # Percorre todos os arquivos da pasta\n",
        "    for nome_arquivo in os.listdir(pasta):\n",
        "        caminho_arquivo = os.path.join(pasta, nome_arquivo)\n",
        "\n",
        "        # Verifica se é um arquivo\n",
        "        if os.path.isfile(caminho_arquivo):\n",
        "            # Deleta o arquivo\n",
        "            os.remove(caminho_arquivo)\n",
        "            print(f\"Arquivo deletado: {caminho_arquivo}\")\n",
        "\n",
        "    print(f\"Todos os arquivos da pasta {pasta} foram deletados.\")\n",
        "\n",
        "deletar_arquivos_pasta('GLM')\n",
        "deletar_arquivos_pasta('Samples')"
      ],
      "metadata": {
        "id": "y8PDSTsjrrGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após isso é declarada a função de downloaad dos dados NetCDF."
      ],
      "metadata": {
        "id": "LBPh0GpzsVhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_GLM(yyyymmddhhmnss, path_dest, bucket_name):\n",
        "\n",
        "  os.makedirs(path_dest, exist_ok=True)\n",
        "\n",
        "  year = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%Y')\n",
        "  day_of_year = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%j')\n",
        "  hour = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%H')\n",
        "  min = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%M')\n",
        "  seg = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%S')\n",
        "\n",
        "  # Initializes the S3 client\n",
        "  s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "  # File structure\n",
        "  product_name = \"GLM-L2-LCFA\"\n",
        "  prefix = f'{product_name}/{year}/{day_of_year}/{hour}/OR_{product_name}_G16_s{year}{day_of_year}{hour}{min}{seg}'\n",
        "\n",
        "  # Seach for the file on the server\n",
        "  s3_result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
        "\n",
        "  # Check if there are files available\n",
        "  if 'Contents' not in s3_result:\n",
        "    print(f'No files found for the date: {yyyymmddhhmnss}, Product-{product_name}')\n",
        "    return -1\n",
        "  else:\n",
        "    for obj in s3_result['Contents']:\n",
        "      key = obj['Key']\n",
        "      file_name = key.split('/')[-1].split('.')[0]\n",
        "      if os.path.exists(f'{path_dest}/{file_name}.nc'):\n",
        "        print(f'File {path_dest}/{file_name}.nc exists')\n",
        "      else:\n",
        "        print(f'Downloading file {path_dest}/{file_name}.nc')\n",
        "        s3_client.download_file(bucket_name, key, f'{path_dest}/{file_name}.nc')\n",
        "  return f'{file_name}'"
      ],
      "metadata": {
        "id": "Atyvgy43sjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para utilizar os dados em um intervalo de tempo de 24 horas até o momento que o código é executado, utilizamos:"
      ],
      "metadata": {
        "id": "R68woWNEsrBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple:\n",
        "lat = -10\n",
        "lon = -50\n",
        "\n",
        "# Desired data:\n",
        "input = \"Samples\"; os.makedirs(input, exist_ok=True)\n",
        "output = \"GLM\"; os.makedirs(output, exist_ok=True)\n",
        "\n",
        "\n",
        "#Dados no tempo real\n",
        "fuso = pytz.timezone('America/Sao_Paulo')\n",
        "now = datetime.now(fuso)\n",
        "\n",
        "final_day = now.day\n",
        "inicial_day = now - timedelta(days=1)\n",
        "month = now.month\n",
        "year = now.year\n",
        "hours = now.hour\n",
        "minutes = now.minute\n",
        "seconds = now.second\n",
        "bucket_name = 'noaa-goes16'\n",
        "\n",
        "date_ini = str(datetime(year, month, inicial_day.day, hours, minutes))\n",
        "date_end = str(datetime(year, month, final_day, hours, minutes))\n",
        "primeiro = True\n"
      ],
      "metadata": {
        "id": "7aURSr2PtM6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma mudança essencial foi a de alterar os intervalos de tempo em que os dados eram captados, o GOES-R capta informações a cada 20 segundos, o que leva a uma execução com muitos dados, chegando a captar mais de 400 mil informações. Assim foi alterado o timedelta no final do loop para ele captar informações a cada 1 hora."
      ],
      "metadata": {
        "id": "OX453BfUtval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while (date_ini <= date_end):\n",
        "    # Get the GLM Data\n",
        "    yyyymmddhhmnss = datetime.strptime(date_ini, '%Y-%m-%d %H:%M:%S').strftime('%Y%m%d%H%M%S')\n",
        "    fileGLM = download_GLM(yyyymmddhhmnss, input, bucket_name)\n",
        "    glm = Dataset(f'{input}/{fileGLM}.nc')\n",
        "\n",
        "    f_lats = glm.variables['flash_lat'][:]\n",
        "    f_lons = glm.variables['flash_lon'][:]\n",
        "\n",
        "    if (primeiro):\n",
        "        df_anterior = pd.DataFrame({\"lat\": f_lats, \"lon\": f_lons, \"time\": datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%Y-%m-%d %H:%M:%S')})\n",
        "        primeiro = False\n",
        "\n",
        "    else:\n",
        "      df_1 = pd.DataFrame({\"lat\": f_lats, \"lon\": f_lons, \"time\": datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%Y-%m-%d %H:%M:%S')})\n",
        "      df = pd.concat([df_anterior, df_1], ignore_index=True)\n",
        "      df_anterior = df\n",
        "\n",
        "    date_ini = str(datetime.strptime(date_ini, '%Y-%m-%d %H:%M:%S') + timedelta(hours=1))\n",
        "\n",
        "df.to_csv(f'{output}/flashs.csv')\n",
        "\n",
        "df = pd.read_csv(f'{output}/flashs.csv')\n",
        "\n",
        "# Looking for a lightning in the user's region\n",
        "for i in range(len(df['lat'])):\n",
        "    if (df['lat'][i] <= (lat + 0.5) and df['lat'][i] >= (lat - 0.5)) and (df['lon'][i] <= (lon + 0.5) and df['lon'][i] >= (lon - 0.5)):\n",
        "        print(\"found lightning\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oj1gzKdhuX6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainda sim, a contagem final de dados fica por volta de 10 mil dados, o que ainda é muito para o cenário estudado. Então foi criada uma função para captar aleatoriamente 150 registros dos captados, esse parametro é usado como exemplo e pode ser alterado."
      ],
      "metadata": {
        "id": "menkwnCQud0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtrar_dados(arquivo_csv, num_registros, arquivo_saida):\n",
        "    registros_selecionados = []\n",
        "    with open(arquivo_csv, 'r') as arquivo:\n",
        "        leitor_csv = csv.reader(arquivo)\n",
        "        cabeçalho = next(leitor_csv)  # Lê o cabeçalho do arquivo CSV\n",
        "\n",
        "        # Lê todos os registros do arquivo e armazena-os em uma lista\n",
        "        todos_registros = list(leitor_csv)\n",
        "\n",
        "        # Verifica se o número de registros solicitado é maior do que o número total de registros\n",
        "        if num_registros > len(todos_registros):\n",
        "            num_registros = len(todos_registros)\n",
        "\n",
        "        # Seleciona aleatoriamente os registros\n",
        "        registros_selecionados = random.sample(todos_registros, num_registros)\n",
        "\n",
        "    # Remove a primeira coluna dos registros selecionados para retirar os indices que não estão sequenciais devido\n",
        "    # a escolha randomica\n",
        "    registros_selecionados = [registro[1:] for registro in registros_selecionados]\n",
        "\n",
        "    # Adiciona o cabeçalho aos registros selecionados\n",
        "    registros_selecionados.insert(0, cabeçalho[1:])\n",
        "\n",
        "    # Obtém o diretório do arquivo de saída\n",
        "    diretorio_saida = os.path.dirname(arquivo_saida)\n",
        "\n",
        "    # Cria o diretório de saída se ele não existir\n",
        "    if not os.path.exists(diretorio_saida):\n",
        "        os.makedirs(diretorio_saida)\n",
        "\n",
        "    # Salva os registros selecionados em um novo arquivo CSV\n",
        "    with open(arquivo_saida, 'w', newline='') as arquivo_saida_csv:\n",
        "        escritor_csv = csv.writer(arquivo_saida_csv)\n",
        "        escritor_csv.writerows(registros_selecionados)\n",
        "\n",
        "\n",
        "    print(f\"Registros selecionados salvos com sucesso no arquivo: {arquivo_saida}\")\n"
      ],
      "metadata": {
        "id": "pnAxYtGiu_au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra necessidade encontrada foi a de achar os valores mínimos e máximos da primeira e segunda coluna (latitude e longitude) para poder usar esses como padrões para a normalização da visualização de dados da composição audiovisual. Para isso foi criado a função que capta esses metadados."
      ],
      "metadata": {
        "id": "RlWG361UvFuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadados(arquivo_csv):\n",
        "    # Ler o arquivo CSV e extrair os valores das colunas\n",
        "    primeira_coluna = []\n",
        "    segunda_coluna = []\n",
        "    with open(arquivo_csv, 'r') as arquivo:\n",
        "        leitor_csv = csv.reader(arquivo)\n",
        "        # Ignorar o cabeçalho\n",
        "        next(leitor_csv)\n",
        "        # Ler as colunas\n",
        "        for linha in leitor_csv:\n",
        "            primeira_coluna.append(float(linha[0]))\n",
        "            segunda_coluna.append(float(linha[1]))\n",
        "\n",
        "    # Criar um dicionário com os metadados\n",
        "    metadados = {\n",
        "        'maior_valor_primeira_coluna': max(primeira_coluna),\n",
        "        'menor_valor_primeira_coluna': min(primeira_coluna),\n",
        "        'maior_valor_segunda_coluna': max(segunda_coluna),\n",
        "        'menor_valor_segunda_coluna': min(segunda_coluna)\n",
        "    }\n",
        "\n",
        "\n",
        "    print(\"Metadados dos limites de latitude e longitude encontrados.\")\n",
        "\n",
        "    #Printa os metadados\n",
        "    for valor in metadados.values():\n",
        "      print(valor)"
      ],
      "metadata": {
        "id": "7OEYIpvYvjVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passagem de parametros e chamada da função:\n"
      ],
      "metadata": {
        "id": "z2suybqvvpae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_csv = 'GLM/flashs.csv'\n",
        "num_registros_selecionados = 150\n",
        "arquivo_saida = 'GLM/flashs_filtro.csv'\n",
        "\n",
        "filtrar_dados(arquivo_csv, num_registros_selecionados, arquivo_saida)\n",
        "metadados(arquivo_saida)"
      ],
      "metadata": {
        "id": "r7vCPRDwvvKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O que pode ser aprimorado?"
      ],
      "metadata": {
        "id": "xB99Pr48vy3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por enquanto, é necessário captar a longitude e latitude exata do usuário para acrescentarmos nesse script. É interessante também formular uma lógica em cima de qual informações são mais importantes para o contexto, já que muitos dados são deixados de lado visando um meio artístico e menos técnico."
      ],
      "metadata": {
        "id": "NTN6Azyov5Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo foi retirado e manipulado a partir de scripts que podem ser encontrados em: https://github.com/isabellarigue/GaiaSenses/blob/main/docs/glm_csv.ipynb e https://geonetcast.wordpress.com/2021/02/25/vlab-processamento-de-dados-de-satelites-geoestacionarios-pre-curso/"
      ],
      "metadata": {
        "id": "h-6DeGeYpSiU"
      }
    }
  ]
}